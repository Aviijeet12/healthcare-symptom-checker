# Backend environment variables
# IMPORTANT: never commit real keys. Put your real key only in healthcare-backend/.env (gitignored)

# Hugging Face (recommended)
HF_API_KEY=
# Example: meta-llama/Llama-3.1-8B-Instruct
HF_MODEL_NAME=

# Optional generation controls
HF_TEMPERATURE=0.2
HF_MAX_TOKENS=512

# Optional: override Hugging Face router base URL
# Default: https://router.huggingface.co/v1
HF_BASE_URL=

# Optional: maximum time to wait for a model that is loading (ms)
HF_WAIT_FOR_MODEL_MAX_MS=25000

# (Optional) Legacy Gemini configuration (only used if HF is not configured)
# GEMINI_API_KEY=
# GEMINI_MODEL=gemini-1.5-flash
# GEMINI_API_BASE_URL=https://generativelanguage.googleapis.com/v1beta

# If enabled (default), backend returns a safe fallback response when the AI provider is unavailable
RETURN_FALLBACK_ON_LLM_ERROR=1

# Redis (optional, for deterministic local testing)
# Start via: docker compose up -d redis
REDIS_URL=redis://localhost:6379/0
REDIS_SEED_PREFIX=symptom-checker:seed

# If enabled, backend will return seeded results from Redis when a matching symptom entry exists.
REDIS_SEEDED_ANALYSIS_ENABLED=0
